{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below if you are using Google Colab to mount your Google Drive in your Colab instance. Adjust the path to the files in your Google Drive as needed if it differs.\n",
    "\n",
    "If you do not use Google Colab, running the cell will simply do nothing, so do not worry about it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive/')\n",
    "    %cd 'drive/My Drive/Colab Notebooks/08_Association'\n",
    "except ImportError as e:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exercise 8: Association Analysis\n",
    "\n",
    "### 8.1. Analyzing the Shopping Basket Data Set\n",
    "\n",
    "#### 8.1.1.\tThe Shopping Basket data set is provided as an Excel file on the website. Load the data set with the read_excel function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#%pip install -q -U Orange3-Associate\n",
    "import pandas as pd\n",
    "shopping = pd.read_excel('ShoppingBaskets.xls')\n",
    "shopping.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "shopping_data = shopping.drop('BasketNo', axis=1)\n",
    "shopping_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### 8.1.2.\tMine frequent item sets from the data set using the frequent_itemsets function (support = 0.2). Which items are usually bought together with the laptop, the netbook and the printer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from orangecontrib.associate.fpgrowth import *\n",
    "import pandas as pd\n",
    "\n",
    "# calculate frequent itemsets\n",
    "\n",
    "\n",
    "# store results in dataframe\n",
    "\n",
    "# show frequent itemsets with the laptop\n",
    "\n",
    "\n",
    "# show frequent itemsets with the netbook\n",
    "\n",
    "\n",
    "# show frequent itemsets with the printer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### 8.1.3.\tCreate association rules from the frequent item sets using the association_rules function with a confidence of 0.70. What do the rules tell you about the relationship between Asus EeePC netbooks, 2 GB DDR3 RAM extensions and Netbook Schutzhüllen? What do the lift values (compute it with the rules_stats function) tell you about the interestingness of the rules?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# create association rules\n",
    "\n",
    "\n",
    "# calculate statistics and store results in a dataframe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 8.2. Finding Frequent Patterns in the Adult Data Set\n",
    "#### 8.2.1.\tImport the Adult-tweaked data set and print a description of the dataset using the describe function. The Adult-tweaked data set is provided on the website as an ARFF file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import arff\n",
    "adult_arff_data, adult_arff_meta = arff.loadarff(open('adult-dataset-tweaked.arff', 'r'))\n",
    "adult = pd.DataFrame(adult_arff_data)\n",
    "adult = adult.applymap(lambda x: x.decode('utf8') if hasattr(x, 'decode') else x)\n",
    "adult.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### 8.2.2.\tPrepare the data set for Frequent Pattern Mining by: 1. reducing the size of the data set to 5000 examples using sampling; 2. Using a subset of attributes: age, workclass, education, occupation, race, sex, hours-per-week, native-country and class; 3. discretizing the attributes age and hours-per-week into three user defined ranges (think about ranges that could make sense for the attributes). How many attributes does the resulting data set have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# draw a random sample\n",
    "adult_sample = adult.sample(n=5000, random_state=14990)\n",
    "\n",
    "# discretize age\n",
    "\n",
    "\n",
    "# discretize hours per week\n",
    "\n",
    "\n",
    "# select attributes\n",
    "\n",
    "\n",
    "# apply one-hot encoding\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### 7.2.3.\tFind frequent item sets that have a support above 0.2. What can you learn from these item sets about the people how earn less than 50K a year? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate frequent itemsets\n",
    "\n",
    "\n",
    "# store results in a dataframe\n",
    "\n",
    "\n",
    "# show only those frequent itemsets that include the item 'class_<=50K'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can learn that people who are earning less than 50K are most likely from the United-States and/or white and/or work 20-45 hours per week and/or are from the Private sector (workclass=private). At this point we do not know about the confidence of these combinations but we know how often they appear together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### 8.2.4.\tGiven the large number of examples and the low min-support threshold, the number of frequent item sets containing education attribute is surprisingly low. Moreover, only one value for this attribute is present in the resulting frequent item sets. Why is this the case? How could you aggregate the data to change this without losing too much information? Also look at the native-country attribute, and think of a possible aggregation for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge categorical values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# apply one-hot encoding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# calculate frequent itemsets\n",
    "\n",
    "\n",
    "# store results in dataframe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### 8.2.5.\tRestrict the frequent item sets to the ones containing “class = >50K” and lower the support so that a decent number of item sets is discovered. What can you learn from these item sets about the people how earn more than 50K a year? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show only those frequent itemsets that include the item 'class_>50K'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 8.3. Mining Association Rules from the Adult Data Set\n",
    "#### 8.3.1.\tCreate association rules from the frequent item sets from exercise 7.2.3 using the association_rules function. Which rules do you consider interesting? Consider both = >50K and <50K classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate frequent itemsets\n",
    "\n",
    "\n",
    "# calculate association rules\n",
    "\n",
    "\n",
    "# calculate statistics and store results in a dataframe\n",
    "\n",
    "\n",
    "# show rules for <=50K\n",
    "\n",
    "\n",
    "# show rules for >50K\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### 8.3.2.\tNow we want to focus on the relationship of the occupation, education and income of immigrants: instead of sampling the data set, filter the data set: native-country != United-States. Which rules do you consider interesting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter data\n",
    "\n",
    "\n",
    "# select attributes and discretize numeric values\n",
    "\n",
    "\n",
    "# apply one-hot encoding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# calculate frequent itemsets\n",
    "\n",
    "\n",
    "# calculate association rules\n",
    "\n",
    "\n",
    "# calculate statistics and store results in a dataframe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ChatGPT Bonus Exercises\n",
    "Reminder: Do not take the answers of ChatGPT at face value! Always cross-check with lecture slides, literature and/or the teaching staff!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C.1. Discuss Metrics and Quality of Assocation Rules\n",
    "* Discuss Association Analysis (Association Rule Mining) with ChatGPT. How are the metrics support, confidence, and lift used to evaluate the quality of discovered association rules? Discuss the following three scenarios and identify the scenario that likely indicates an interesting rule:\n",
    "    1. Low support and confidence as well as a lift close to 1.\n",
    "    2. High support and low confidence and a lift close to 1.\n",
    "    3. High support and confidence as well as a lift greater than 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C.2. Generate code to compare the Apriori algorithm and the FPgrowth algorithm\n",
    "* In the lecture you learned about the Apriori algorithm for generating frequent itemsets. In the exercise we have used the FPgrowth algorithm instead to produce such itemsets. Have ChatGPT explain the difference between both algorithms to you. Ask ChatGPT for some code to produce frequent itemsets using the apriori algorithm with the orange3 library.\n",
    "\n",
    "* Will both algorithms produce the same itemsets? Discuss this question with ChatGPT and ask for code that applies both algorithms using the orange3 library and subsequently compares the generated frequent item sets. Apply the code for one of the datasets from the exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C.3. Self-Assessment\n",
    "* Ask ChatGPT to create a pen and paper exercise by giving you some transaction items to practice to calculate the confidence of some association rules.\n",
    "* Ask ChatGPT to generate some multiple-choice questions for graduate students related to association analysis. Include questions regarding pre-processing nominal and continuous features and shortcomings of the metrics support, confidence and lift that are used to assess the relevancy of association rules."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below if you are using Google Colab to mount your Google Drive in your Colab instance. Adjust the path to the files in your Google Drive as needed if it differs.\n",
    "\n",
    "If you do not use Google Colab, running the cell will simply do nothing, so do not worry about it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive/')\n",
    "    %cd 'drive/My Drive/Colab Notebooks/05_Classification'\n",
    "except ImportError as e:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exercise 5: Classification\n",
    "\n",
    "### 5.1. Learning a classifier for the Iris Data Set\n",
    "\n",
    "In the last exercise, you have learned lazy classification models for the Iris dataset. Now try a Decision Tree based approach with 10-fold cross-validation. Use a pipeline to perform some preprocessing before learning or applying the decision tree classifier.\n",
    "\n",
    "#### 5.1.1.\tDiscretise the Iris data set into three bins. Then use the DecisionTreeClassifier with a 10-fold stratified cross validation and compute the accuracy. Afterwards plot the decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "## Windows users: either add the path to graphviz' dot.exe to your PATH variable \n",
    "## OR comment in the 2 lines below (may have to change path):\n",
    "\n",
    "# import os\n",
    "# os.environ['PATH'] += ';C:\\\\Program Files (x86)\\\\Graphviz2.38\\\\bin'\n",
    "\n",
    "iris = pd.read_csv(\"iris.csv\")\n",
    "iris_data = iris[['SepalLength', 'SepalWidth', 'PetalLength', 'PetalWidth']]\n",
    "iris_target = iris['Name']\n",
    "\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 5.1.2.\tRemove the discretization and adjust the max_depth parameter of DecisionTreeClassifier to increase the accuracy. Does the accuracy change? Compare the complexity of the two models. Which model should be preferred according to Occam’s razor?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exercise 5: Classification\n",
    "\n",
    "### 5.2. Parameter optimization\n",
    "In Exercise 4.1 we have used the German credit data set from the UCI data set library (http://archive.ics.uci.edu/ml/index.html), which describes the customers of a bank with respect to whether they should get a bank credit or not. The data set is provided as credit-g.arff file in ILIAS.\n",
    "\n",
    "#### 5.2.1.\t(recap) Go back to the results of exercise 4.1.4. Re-run the classifiers with their default parameter settings.\n",
    "- Used the 10-fold validation approach.\n",
    "- Balanced the training set multiplying the “bad customer” examples. \n",
    "- Evaluated the results, setting up your cost matrix to ((0,100)(1,0)) – that is, you assumed you will lose 1 Unit if you refuse a credit to a good customer, but that you lose 100 Units if you give a bad customer a credit.\n",
    "\n",
    "Rerun your process to get the performance results. Now additionally use a  Decision Tree Classifier. How does it perform? What were the default parameters of the Decision Tree?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we load the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.io import arff\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "credit_arff_data, credit_arff_meta = arff.loadarff(open('credit-g.arff', 'r'))\n",
    "credit_data = pd.DataFrame(credit_arff_data)\n",
    "\n",
    "# select all columns of type object\n",
    "columns_with_binary_strings = credit_data.select_dtypes('object').columns.values\n",
    "\n",
    "# decode the values of these columns using utf-8\n",
    "credit_data[columns_with_binary_strings] = credit_data[columns_with_binary_strings].apply(lambda x: x.str.decode(\"utf-8\"))\n",
    "credit_target = credit_data['class']\n",
    "credit_data = credit_data.drop(columns='class')\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "credit_target = label_encoder.fit_transform(credit_target)\n",
    "label_names=['bad','good']\n",
    "label_order=label_encoder.transform(label_names)\n",
    "\n",
    "credit_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we set up a pipeline and evaluate it using cross validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler\n",
    "from sklearn.model_selection import cross_val_predict, StratifiedKFold\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "numeric_features = ['duration', 'credit_amount', 'installment_commitment', 'residence_since', 'age', 'existing_credits', 'num_dependents']\n",
    "categorical_features = ['credit_history', 'purpose', 'personal_status', 'other_parties', 'property_magnitude', 'other_payment_plans', 'housing', 'job', 'own_telephone', 'foreign_worker']\n",
    "ordinal_features = [ 'checking_status', 'savings_status', 'employment']\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numeric_features),\n",
    "        ('cat', OneHotEncoder(), categorical_features),\n",
    "        ('ord', OrdinalEncoder(categories=[\n",
    "            [ 'no checking', '<0', '0<=X<200', '>=200' ],\n",
    "            [ 'no known savings', '<100', '100<=X<500', '500<=X<1000', '>=1000' ],\n",
    "            [ 'unemployed', '<1', '1<=X<4', '4<=X<7', '>=7' ]\n",
    "        ]), ordinal_features)])\n",
    "\n",
    "def cost_function(y_true, y_pred): \n",
    "    cm = confusion_matrix(y_true, y_pred, labels=label_order)\n",
    "    return cm[0][1] * 100 + cm[1][0] * 1\n",
    "\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we fit the pipeline to the dataset and plot the decision tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import graphviz \n",
    "from sklearn import tree\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "\n",
    "## Windows users: either add the path to graphviz' dot.exe to your PATH variable \n",
    "## OR comment in the 2 lines below (may have to change path):\n",
    "\n",
    "# import os\n",
    "# os.environ['PATH'] += ';C:\\\\Program Files (x86)\\\\Graphviz2.38\\\\bin'\n",
    "\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 5.2.2.\tNow, try to find a more appropriate configuration for the Decision Tree classifier. Use the GridSearchCV from scikit-learn. \n",
    "\n",
    "Try the following parameters of the Decision Tree:\n",
    "- criterion: ['gini', 'entropy']\n",
    "- 'max_depth': [2, 3, 4, 5, None]\n",
    "- 'min_samples_split' :[2,3,4,5]\n",
    "\n",
    "You should come up with 48 (2 x 6 x 4) combinations.\n",
    "\n",
    "What is the best configuration for the data set and the classification approach? \n",
    "\n",
    "Note: The grid search can take some time. You can use the ```n_jobs=-1``` parameter setting for the ```cross_val_predict()``` function to enable parallel processing (all CPU cores will be used)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "# define the parameter grid\n",
    "parameters = {\n",
    "    'estimator__criterion':['gini', 'entropy'],\n",
    "    #TODO: add more parameters\n",
    "}\n",
    "\n",
    "# define the folds for the cross validation\n",
    "\n",
    "# create a scorer for the grid search\n",
    "cost_score = make_scorer(cost_function, greater_is_better=False)\n",
    "\n",
    "# create the grid search estimator\n",
    "\n",
    "# cross-validate\n",
    "\n",
    "# calculate costs\n",
    "\n",
    "# fit the grid search (= determine the optimal parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 5.1.4.\tHow does the optimal decision tree differ from the one you have learned in 5.2.1?\n",
    "Plot the optimised tree!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below if you are using Google Colab to mount your Google Drive in your Colab instance. Adjust the path to the files in your Google Drive as needed if it differs.\n",
    "\n",
    "If you do not use Google Colab, running the cell will simply do nothing, so do not worry about it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive/')\n",
    "    %cd 'drive/My Drive/Colab Notebooks/06_Regression'\n",
    "except ImportError as e:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Regression\n",
    "\n",
    "- Classification predicts a **categorical** value\n",
    "    - a finite set of values\n",
    "- Regression predicts a **numerical** value\n",
    "    - a possibly infinite set of values\n",
    "    - can be interpolating or extrapolating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Regression Estimators\n",
    "\n",
    "Regression estimators work in the same way as classification estimators in scikit-learn:\n",
    "\n",
    "Linear Regression Models:\n",
    "- [LinearRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html)\n",
    "- [Ridge](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html#sklearn.linear_model.Ridge)\n",
    "\n",
    "K-Nearest Neighbor Regression:\n",
    "- [KNeighborsRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsRegressor.html)\n",
    "\n",
    "Decision Tree Regression:\n",
    "- [DecisionTreeRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html)\n",
    "\n",
    "Neural Network Regression:\n",
    "- [MLPRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We will use a simple dataset about fish for the introduction of regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "fish = pd.read_csv('fish.csv')\n",
    "fish.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Lets have a look at some plots to determine how the weight and length of the fish are related to the age:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# create a list of all columns that we are considering\n",
    "features = [ 'age', 'temp', 'weight', 'length' ]\n",
    "\n",
    "# create all combinations of considered columns\n",
    "combinations = itertools.combinations(features, 2)\n",
    "\n",
    "# create a figure and specify its size\n",
    "fig = plt.figure(figsize=(15,10))\n",
    "\n",
    "# go through all combinations and create one plot for each\n",
    "figure_index = 1\n",
    "for combination in combinations:\n",
    "    # add a sub plot to the figure\n",
    "    axs = fig.add_subplot(2,3,figure_index)\n",
    "    \n",
    "    # plot the feature combination\n",
    "    axs.scatter(fish[combination[0]], fish[combination[1]])\n",
    "    \n",
    "    # set the axis labels of the current sub plot\n",
    "    axs.set_xlabel(combination[0])\n",
    "    axs.set_ylabel(combination[1])\n",
    "        \n",
    "    # increase the figure index (otherwise all plots are drawn in the first subplot)\n",
    "    figure_index+=1\n",
    "\n",
    "    \n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "It seems that there is a linear relationship between age and weight. We can fit a linear regression and add it to the plot. We use the age as a feature and the weight as target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# separate features and target variable\n",
    "weight = fish['weight']\n",
    "\n",
    "# special case: we only have one feature, so we must reshape the data here\n",
    "features = fish['age'].values.reshape(-1, 1)\n",
    "\n",
    "# create a train/test split\n",
    "weight_train, weight_test, weight_target_train, weight_target_test = train_test_split(\n",
    "    features, weight, test_size=0.4, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Now let's fit a linear regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# create and fit a linear regression\n",
    "weight_estimator = LinearRegression()\n",
    "\n",
    "weight_estimator.fit(weight_train, weight_target_train)\n",
    "\n",
    "# plot the original values\n",
    "plt.scatter(weight_train, weight_target_train, c='green', label='train')\n",
    "plt.scatter(weight_test, weight_target_test, c='blue', label='test')\n",
    "\n",
    "# plot the predicted values\n",
    "plt.plot(fish['age'], weight_estimator.predict(features),c='red', label='prediction')\n",
    "\n",
    "plt.xlabel('age')\n",
    "plt.ylabel('weight')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# print the model that was fitted (the regression formula)\n",
    "print(\"weight = {}*age + {}\".format(weight_estimator.coef_[0], weight_estimator.intercept_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Now lets see if that also works for the length. We use the age as feature again and the length as target variable now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate features and target variable\n",
    "length = fish['length']\n",
    "\n",
    "# create a train/test split\n",
    "length_train, length_test, length_target_train, length_target_test = train_test_split(\n",
    "    features, length, test_size=0.4, random_state=42)\n",
    "\n",
    "# create and fit a linear regression\n",
    "length_estimator = LinearRegression()\n",
    "length_estimator.fit(length_train, length_target_train)\n",
    "\n",
    "# plot the original values\n",
    "plt.scatter(length_train, length_target_train, c='green', label='train')\n",
    "plt.scatter(length_test, length_target_test, c='blue', label='test')\n",
    "\n",
    "# plot the predicted values\n",
    "plt.plot(fish['age'], length_estimator.predict(features),c='red', label='prediction')\n",
    "\n",
    "plt.xlabel('age')\n",
    "plt.ylabel('length')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# print the model that was fitted (the regression formula)\n",
    "print(\"length = {}*age + {}\".format(length_estimator.coef_[0], length_estimator.intercept_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Polynomial Features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fitted regression does not really match the data that we see. It seems that we need a polynomial regression here. We can fit such a regression by using a [```PolynomialFeatures``` transformer](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html) that generates all possible feature combinations for the polynomial that we want to fit. On these transformed features, we can then use the linear regression again to fit our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# create a transformer that generates polynomial features\n",
    "transformer = PolynomialFeatures(degree=2, include_bias=False)\n",
    "estimator = LinearRegression()\n",
    "pipeline = Pipeline([ ('transformer', transformer), ('estimator', estimator)])\n",
    "pipeline.fit(length_train, length_target_train)\n",
    "\n",
    "prediction = pipeline.predict(features)\n",
    "\n",
    "# plot the original values\n",
    "plt.scatter(length_train, length_target_train, c='green', label='train')\n",
    "plt.scatter(length_test, length_target_test, c='blue', label='test')\n",
    "\n",
    "# create a new dataframe that contains the age and the predictions\n",
    "d = fish[['age']]\n",
    "d = d.assign(prediction=prediction)\n",
    "\n",
    "# sort the data before plotting it\n",
    "d = d.sort_values(by='age')\n",
    "\n",
    "# plot the predicted values\n",
    "plt.plot(d['age'], d['prediction'], c='red', label='prediction')\n",
    "\n",
    "plt.xlabel('age')\n",
    "plt.ylabel('length')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# print the model that was fitted (the regression formula)\n",
    "print(\"length = \", end='')\n",
    "for i, f in enumerate(pipeline.named_steps['transformer'].get_feature_names_out(['age'])):\n",
    "    if i > 0:\n",
    "        print(\" + \", end='')\n",
    "    print(\"{}*{}\".format(pipeline.named_steps['estimator'].coef_[i], f), end='')\n",
    "print(\" + {}\".format(pipeline.named_steps['estimator'].intercept_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Evaluation\n",
    "\n",
    "With a continuous target variable, it does not make sense to count how often we predicted the exact correct value. The measures used for regression rather check how close our prediction is to the correct value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from math import sqrt\n",
    "\n",
    "# predict the values for the test data\n",
    "predictions = pipeline.predict(length_test)\n",
    "\n",
    "# evaluate using different measures\n",
    "mse = mean_squared_error(length_target_test, predictions)\n",
    "r2 = r2_score(length_target_test, predictions)\n",
    "\n",
    "print(\"MSE:\", mse)\n",
    "print(\"RMSE:\", sqrt(mse))\n",
    "print(\"R^2:\", r2)\n",
    "\n",
    "# print the model that was fitted (the regression formula)\n",
    "print(\"length = \", end='')\n",
    "for i, f in enumerate(pipeline.named_steps['transformer'].get_feature_names_out(['age'])):\n",
    "    if i > 0:\n",
    "        print(\" + \", end='')\n",
    "    print(\"{}*{}\".format(pipeline.named_steps['estimator'].coef_[i], f), end='')\n",
    "print(\" + {}\".format(pipeline.named_steps['estimator'].intercept_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Try it  yourself\n",
    "- Task 6.1.1\n",
    "![xkcd comic](https://imgs.xkcd.com/comics/extrapolating.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Feature Selection\n",
    "\n",
    "- [```f_regression``` function](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.f_regression.html)\n",
    "    - Performs an [F-Test](https://en.wikipedia.org/wiki/F-test) to determine feature importances\n",
    "\n",
    "- [```SelectKBest``` class](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html)\n",
    "     - selects the ```k``` best features according to ```score_func```\n",
    "     \n",
    "- [```SelectFwe``` class](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectFwe.html)\n",
    "    - selects all features with a p-value above ```threshold``` according to ```score_func```\n",
    "\n",
    "- Recursive Feature Elimination: [```RFECV``` class](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFECV.html)\n",
    "    - fits a model (any estimator that provides feature weights) and removes the ```step``` least important features\n",
    "    - uses cross validation to find the optimal number of features\n",
    "    \n",
    "For more details, have a look at the [feature selection documentation of scikit-learn](https://scikit-learn.org/stable/modules/feature_selection.html).\n",
    "\n",
    "For this part, we will use all features in the fish dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# separate features and target variable\n",
    "fish_target = fish['length']\n",
    "fish_data = fish.drop(columns=['length'])\n",
    "\n",
    "# create a train/test split\n",
    "data_train, data_test, target_train, target_test = train_test_split(\n",
    "    fish_data, fish_target,test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's first try an F Regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import f_regression\n",
    "\n",
    "# create a transformer\n",
    "transformer = PolynomialFeatures(degree=2, include_bias=False)\n",
    "\n",
    "# run the F-Test\n",
    "f, pval = f_regression(transformer.fit_transform(data_train), target_train)\n",
    "\n",
    "# prepare a dataframe to inspect the results\n",
    "stat = pd.DataFrame({ 'feature': transformer.get_feature_names_out(fish_data.columns), 'F value': f, 'p value': pval })\n",
    "stat['p value'] = round(stat['p value'], 2)\n",
    "\n",
    "# show the results\n",
    "display(stat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectFwe\n",
    "\n",
    "best = SelectFwe(f_regression, alpha=0.05)\n",
    "transformer = PolynomialFeatures(degree=2, include_bias=False)\n",
    "\n",
    "pipeline = Pipeline([ ('transformer', transformer), ('feature_selection', best), ('estimator', estimator)])\n",
    "\n",
    "# fit the regression on the training data\n",
    "pipeline.fit(data_train, target_train)\n",
    "\n",
    "# predict the values for the test data\n",
    "predictions = pipeline.predict(data_test)\n",
    "\n",
    "# evaluate using different measures\n",
    "mse = mean_squared_error(target_test, predictions)\n",
    "r2 = r2_score(target_test, predictions)\n",
    "\n",
    "print(\"MSE:\", mse)\n",
    "print(\"RMSE:\", sqrt(mse))\n",
    "print(\"R^2:\", r2)\n",
    "\n",
    "# get the selected features\n",
    "selected_features = pipeline.named_steps['feature_selection'].get_support()\n",
    "feature_index = 0\n",
    "\n",
    "# print the model that was fitted (the regression formula)\n",
    "for i, f in enumerate(pipeline.named_steps['transformer'].get_feature_names_out(fish_data.columns)):\n",
    "    # check if the feature was selected\n",
    "    if selected_features[i]:\n",
    "        if i > 0:\n",
    "            print(\" + \", end='')\n",
    "        print(\"{}*{}\".format(pipeline.named_steps['estimator'].coef_[feature_index], f), end='')\n",
    "        feature_index += 1\n",
    "print(\" + {}\".format(pipeline.named_steps['estimator'].intercept_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Of course we can also make feature selection a part of the sklearn pipeline and have it automatically train a model with the best found features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# create a transformer and linear regression\n",
    "transformer = PolynomialFeatures(degree=2, include_bias=False)\n",
    "scaler = StandardScaler()\n",
    "estimator_feature_selection = LinearRegression()\n",
    "estimator_classification = DecisionTreeRegressor()\n",
    "\n",
    "# create the feature selection estimator\n",
    "feature_selection = RFECV(estimator_feature_selection, cv=10)\n",
    "\n",
    "# setup the pipeline\n",
    "pipeline = Pipeline([ ('transformer', transformer), ('scaler', scaler), ('feature_selection', feature_selection), ('classification', estimator_classification)])\n",
    "\n",
    "# fit pipeline\n",
    "pipeline.fit(data_train, target_train)\n",
    "\n",
    "# predict the values for the test data\n",
    "predictions = pipeline.predict(data_test)\n",
    "\n",
    "# evaluate\n",
    "mse = mean_squared_error(target_test, predictions)\n",
    "r2 = r2_score(target_test, predictions)\n",
    "print(\"MSE:\", mse)\n",
    "print(\"RMSE:\", sqrt(mse))\n",
    "print(\"R^2:\", r2)\n",
    "\n",
    "# get the selected features\n",
    "fs = pipeline.named_steps['feature_selection']\n",
    "est = pipeline.named_steps['feature_selection'].estimator_\n",
    "\n",
    "selected_features = fs.get_support()\n",
    "feature_index = 0\n",
    "\n",
    "# print the model that was fitted (the regression formula)\n",
    "print(\"length = \", end='')\n",
    "for i, f in enumerate(pipeline.named_steps['transformer'].get_feature_names_out(fish_data.columns)):\n",
    "    # check if the feature was selected\n",
    "    if selected_features[i]:\n",
    "        if i > 0:\n",
    "            print(\" + \", end='')\n",
    "        print(\"{}*{}\".format(est.coef_[feature_index], f), end='')\n",
    "        feature_index += 1\n",
    "print(\" + {}\".format(est.intercept_))"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

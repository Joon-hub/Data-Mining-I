{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below if you are using Google Colab to mount your Google Drive in your Colab instance. Adjust the path to the files in your Google Drive as needed if it differs.\n",
    "\n",
    "If you do not use Google Colab, running the cell will simply do nothing, so do not worry about it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive/')\n",
    "    %cd 'drive/My Drive/Colab Notebooks/07_TextMining'\n",
    "except ImportError as e:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exercise 7: Text Mining\n",
    "\n",
    "### 7.1. Which documents are similar?\n",
    "\n",
    "#### 7.1.1. The file documents.zip is provided in ILIAS and contains three corpora. Load and vectorize the 4-documents corpus using the load_files function. How many different attributes has the generated example set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_files\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "corpus_4_docs = load_files('DataSetEx7', categories=['corpus-4docs'], encoding='utf-8')\n",
    "\n",
    "# create a vectorizer and transform the documents\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### 7.1.2.\tExamine the generated word list. What are the most common words? Look for the three most common words that might be helpful for text mining tasks!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def generate_word_list(X, Y, feature_names, target_names):\n",
    "    d = pd.DataFrame(X.toarray(), columns=feature_names)\n",
    "    doc = d[ d>0 ].count()\n",
    "    d = d.assign(target=Y)\n",
    "    d = d.groupby(by='target').sum()\n",
    "    d = d.transpose()\n",
    "    d.columns = target_names\n",
    "    total = d.sum(axis=1)\n",
    "    d = d.assign(total_occurrences=total)\n",
    "    d = d.assign(document_occurrences=doc)\n",
    "    d = d.sort_values(by='total_occurrences', ascending=False)\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create the word list from the transformed dataset and show it\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##### 7.1.3. Remove stopwords and apply the porter stemmer. By how many attributes do the operators reduce the size of your example set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "import re, string\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "token_pattern = re.compile(r\"(?u)\\b\\w\\w+\\b\")\n",
    "my_stopwords = set(stopwords.words('english'))\n",
    "\n",
    "def tokenize(text):\n",
    "    stems = []\n",
    "    tokens = token_pattern.findall(text)\n",
    "    for item in tokens:\n",
    "        if item not in my_stopwords:\n",
    "            stems.append(stemmer.stem(item))\n",
    "    return stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new vectorizer with stemming and transform the documents again\n",
    "\n",
    "# re-create the word list based on the new vectorizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### 7.1.4.\tCompute the cosine similarity on TF-IDF vectors between the documents with the cosine_similarity function. Which documents are most similar? Can you confirm the judgment of the algorithm by reading the documents?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# create a vectorizer that uses TF-IDF weights\n",
    "\n",
    "\n",
    "# calculate the cosine similarity between all documents and show the results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the two most similar documents\n",
    "\n",
    "#TODO: change the indices to the indices of the most similar documents\n",
    "idx1 = 0\n",
    "idx2 = 0\n",
    "\n",
    "print(corpus_4_docs.data[idx1][:500])\n",
    "print('\\n==================\\n')\n",
    "print(corpus_4_docs.data[idx2][:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### 7.1.5.\tExperiment with different similarity metrics as well as with different vector creation methods. Which combination produces the best similarity scores? \n",
    "\n",
    "for different pairwise distances you can use the [pairwise_distances function](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise_distances.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import *\n",
    "from sklearn.feature_extraction.text import *\n",
    "\n",
    "# create different vectorizers\n",
    "\n",
    "\n",
    "# calcualte the features\n",
    "\n",
    "\n",
    "# calculate different similarity/distance functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 7.2. Cluster the 30-Documents Corpus\n",
    "#### 7.2.1.\tThe 30-documents corpus contains postings from three news groups. Vectorize the 30-documents corpus, remove stopwords and stem the corpus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_30_docs = load_files('DataSetEx7/corpus-30docs',encoding='utf-8')\n",
    "\n",
    "# vectorize the documents\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### 7.2.2. Use K-Means to cluster the corpus.  Compare how many documents from the same category end up in the same cluster!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# run the clusterer\n",
    "\n",
    "\n",
    "# check the results: compare the cluster IDs to the target values (document categories)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### 7.2.3.\tExamine the distribution of frequent words over the three different classes in the word list. Does the distribution give you an idea how you could improve the clustering using any of the prune methods (max_df, min_df)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print the word list sorted by document occurrences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the vectorizer and include pruning based on document frequency of tokens\n",
    "\n",
    "\n",
    "# re-run the clusterer\n",
    "\n",
    "\n",
    "# check if the results improved\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 7.3. Learn a Classifier for the 300-Documents Corpus\n",
    "The 300-documents corpus contains postings from three different news groups. Vectorize\n",
    "the 300-documents corpus and learn a classifier for classifying the postings. Evaluate the\n",
    "classifier using 10-fold X-Validation. Which accuracy does your classifier reach? Increase the\n",
    "performance of your classifier by pruning the document vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "corpus_300_docs = load_files('DataSetEx7/corpus-300docs',encoding='utf-8')\n",
    "\n",
    "class_dist = pd.Series(corpus_300_docs.target).value_counts()\n",
    "plt.bar(class_dist.index, class_dist)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# create a vectorizer\n",
    "\n",
    "# inspect the word list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we create a baseline model with all features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# create a vectorizer for your baseline\n",
    "\n",
    "\n",
    "# define the cross-validation splits\n",
    "cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "# evaluate a baseline model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we test different pruning approaches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# define a pipeline and parameter grid\n",
    "\n",
    "\n",
    "# define the cross-validation splits for the nested CV\n",
    "nested_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# define and evaluate a grid search\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 7.4. Learn a Classifier for the Job Postings\n",
    "#### 7.4.1.\tThe Job Postings corpus contains 500 descriptions of open positions belonging to 30 different job categories. The corpus is provided as an Excel file in ILIAS. Vectorize the corpus  and learn a Naïve Bayes classifier for classifying the job adds. Evaluate the classifying using 10-fold X-Validation. Analyze the classifier performance and the word list. What do you discover? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>JobText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>customer service</td>\n",
       "      <td>OGPlanet (www.ogplanet.com) is an online game ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>customer service</td>\n",
       "      <td>Our company is right now looking for a full-ti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>customer service</td>\n",
       "      <td>&lt;br&gt;\\nARE YOU READY TO TAKE YOUR CAREER TO THE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>customer service</td>\n",
       "      <td>Due to rapid growth, we currently have several...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>customer service</td>\n",
       "      <td>&lt;br&gt;\\nOur growing coffee company is looking fo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Category                                            JobText\n",
       "0  customer service  OGPlanet (www.ogplanet.com) is an online game ...\n",
       "1  customer service  Our company is right now looking for a full-ti...\n",
       "2  customer service  <br>\\nARE YOU READY TO TAKE YOUR CAREER TO THE...\n",
       "3  customer service  Due to rapid growth, we currently have several...\n",
       "4  customer service  <br>\\nOur growing coffee company is looking fo..."
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "job_postings = pd.read_excel('DataSetEx7/JobPostings.xls')\n",
    "job_postings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "job_postings_target = job_postings['Category']\n",
    "job_postings_data = job_postings['JobText']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# plot and inspect the class distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorize the documents and show the word list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### 7.4.2 Experiment with different vector creation and pruning methods as well as different types of classifiers in order to increase the performance. What is highest accuracy that you can reach? Which problem concerning precision and recall does remain?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup and evaluate a baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# create a pipeline and parameter grid\n",
    "\n",
    "\n",
    "# create and evaluate a grid search\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ChatGPT Bonus Exercises\n",
    "Reminder: Do not take the answers of ChatGPT at face value! Always cross-check with lecture slides, literature and/or the teaching staff!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C.1. Discuss about Text Mining and preprocessing steps for textual data\n",
    "* Discuss with ChatGPT the common preprocessing steps used in text mining. Ask it to explain to you a preprocessing step listed that you did not understand. Example: What is POS tagging and how can you use it?\n",
    "* Pre-trained language models such as BERT outperform traditional, bag-of-words based NLP methods on many text classification tasks. Discuss with ChatGPT the potential reasons for the improved performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C.2. Learn about Feature Generation and Feature Selection\n",
    "* Ask ChatGPT how you can generate features from textual data. Further ask it to provide you an example code of generating features from text using TF-IDF. For the same documents, ask it to show you how you can generate features using word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In applications, not all features that are generated are helpful and/or may lead to the “curse of dimensionality”. Ask ChatGPT what the curse of dimensionality is (in the context of text mining) and some common feature selection methods that are used to overcome it. Further ask for an example textual dataset and how to perform feature selection on it. Choose a classification algorithm and learn a model using the data from before feature selection and after and compare the performance of the two models learned. Did feature selection help in this case regarding training time and model performance? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C.3. Self-Assessment\n",
    "* Ask ChatGPT to give you some multiple-choice questions for graduate students related to text mining. Request the correct answers and compare with your own answers.\n",
    "* Ask ChatGPT to generate you some short documents and to give you a paper and pen exercise to calculate binary term occurrence vectors of the documents. Find the most similar documents using Jaccard similarity. Ask ChatGPT to give you the answers when you have finished the exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
